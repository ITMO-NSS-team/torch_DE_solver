{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSO optimizer tutorial.\n",
    "### Burgers equation.\n",
    "$$\\frac{\\partial u}{\\partial t}+ u\\frac{\\partial u}{\\partial x}=\\mu\\frac{\\partial^2 u}{\\partial x^2} $$\n",
    "$$\\mu=0.02/\\pi$$\n",
    "$$x\\in[-1,1]$$\n",
    "$$t\\in[0,1]$$\n",
    "\n",
    "*Initial and boundary conditions*\n",
    "$$u(x, t=0)=-sin(\\pi*x)$$\n",
    "$$u(x=-1, t)=0$$\n",
    "$$u(x=1, t)=0$$\n",
    "\n",
    "import libraries and Solver modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.integrate import quad\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "sys.path.append('../')\n",
    "sys.path.pop()\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('tutorials'), '..')))\n",
    "\n",
    "from tedeous.input_preprocessing import Equation\n",
    "from tedeous.solver import Solver, PSO, Plots, grid_format_prepare\n",
    "from tedeous.solution import Solution\n",
    "from tedeous.device import solver_device, check_device\n",
    "from tedeous.models import mat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building grid, boundary conditions, equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available and used.\n"
     ]
    }
   ],
   "source": [
    "solver_device('cuda')\n",
    "\n",
    "mode = 'autograd'\n",
    "\n",
    "mu = 0.02 / np.pi\n",
    "\n",
    "##grid creation\n",
    "x = torch.linspace(-1, 1, 21)\n",
    "t = torch.linspace(0, 1, 21)\n",
    "\n",
    "grid = grid_format_prepare([x, t], mode=mode).float()\n",
    "\n",
    "##initial cond\n",
    "bnd1 = torch.cartesian_prod(x, torch.tensor([0.])).float()\n",
    "bndval1 = -torch.sin(np.pi * bnd1[:, 0])\n",
    "\n",
    "##boundary cond\n",
    "bnd2 = torch.cartesian_prod(torch.tensor([-1.]), t).float()\n",
    "bndval2 = torch.zeros_like(bnd2[:, 0])\n",
    "\n",
    "##boundary cond\n",
    "bnd3 = torch.cartesian_prod(torch.tensor([1.]), t).float()\n",
    "bndval3 = torch.zeros_like(bnd3[:, 0])\n",
    "\n",
    "## collecting all conditions in:\n",
    "# bnd (boundary points),\n",
    "# bop (boundary opertor if exists),\n",
    "# bndval (boundary value),\n",
    "# var (variable number in system case),\n",
    "# cond-n type ('dirichlet', 'operator', 'periodic', 'data')\n",
    "\n",
    "bconds = [[bnd1, bndval1, 'dirichlet'],\n",
    "            [bnd2, bndval2, 'dirichlet'],\n",
    "            [bnd3, bndval3, 'dirichlet']]\n",
    "\n",
    "## equation part\n",
    "burgers_eq = {\n",
    "    'du/dt**1':\n",
    "        {\n",
    "            'coeff': 1.,\n",
    "            'du/dt': [1],\n",
    "            'pow': 1,\n",
    "            'var': 0\n",
    "        },\n",
    "    '+u*du/dx':\n",
    "        {\n",
    "            'coeff': 1,\n",
    "            'u*du/dx': [[None], [0]],\n",
    "            'pow': [1, 1],\n",
    "            'var': [0, 0]\n",
    "        },\n",
    "    '-mu*d2u/dx2':\n",
    "        {\n",
    "            'coeff': -mu,\n",
    "            'd2u/dx2': [0, 0],\n",
    "            'pow': 1,\n",
    "            'var': 0\n",
    "        }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exact solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact(grid):\n",
    "    mu = 0.02 / np.pi\n",
    "\n",
    "    def f(y):\n",
    "        return np.exp(-np.cos(np.pi * y) / (2 * np.pi * mu))\n",
    "\n",
    "    def integrand1(m, x, t):\n",
    "        return np.sin(np.pi * (x - m)) * f(x - m) * np.exp(-m ** 2 / (4 * mu * t))\n",
    "\n",
    "    def integrand2(m, x, t):\n",
    "        return f(x - m) * np.exp(-m ** 2 / (4 * mu * t))\n",
    "\n",
    "    def u(x, t):\n",
    "        if t == 0:\n",
    "            return -np.sin(np.pi * x)\n",
    "        else:\n",
    "            return -quad(integrand1, -np.inf, np.inf, args=(x, t))[0] / quad(integrand2, -np.inf, np.inf, args=(x, t))[\n",
    "                0]\n",
    "\n",
    "    solution = []\n",
    "    for point in grid:\n",
    "        solution.append(u(point[0].item(), point[1].item()))\n",
    "\n",
    "    return torch.tensor(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When grid, equation, boundary conditions  exist, we should call preprocessing class Equation with method set_strategy and initialize model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "equation = Equation(grid, burgers_eq, bconds).set_strategy(mode)\n",
    "\n",
    "## model part\n",
    "if mode in ('NN', 'autograd'):\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(2, 10),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(10, 10),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(10, 10),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(10, 1)\n",
    "    )\n",
    "else:\n",
    "    model = mat_model(grid, equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we can initialize optimizer, it may be one of torch optimizers or custom form *tedeous.optimizers* module.\n",
    "If you want to run optimizatoin process with default settings,\n",
    "you will be able to set it as string (\"Adam\", \"SGD\", \"LBFGS\", \"PSO\")\n",
    "\n",
    "*Here is main aspects of particle swarm optimizer realization*\n",
    "1. For optimization, the swarm *pop_size* is built based on initial model weights with adding some *variance* (influence on search space)\n",
    "2. Each individual in this swarm represents a candidate solution. At each iteration, the particles in the swarm exchange\n",
    "information and use it to update their positions.\n",
    "3.  Particle $\\theta^t$ at iteration $t$ is changed by three factors: its own velocity inertia $\\beta \\upsilon^t$\n",
    ", its best-known position $p_{best}$ in the search-space, as well as the\n",
    "entire swarm’s best-known position $g_{best}$:\n",
    "$$\\upsilon^{t+1} = \\beta*\\upsilon^{t} + (1-\\beta)*(c1*r1(p_{best} − \\theta^t) + c2*r2(g_{best} − \\theta^t))$$\n",
    "where *c1* and *c2* are the cognitive and social coefficients, respectively, referred to jointly as the behavioral\n",
    "coefficients, and *r1* and *r2* are uniformly distributed random numbers in range (-*variance*, *variance*). Then the particle position is updated as:\n",
    "$$\\theta^{t+1} = \\theta^t + \\upsilon^{t+1}$$\n",
    "4. PSO can be combined with gradient descent to train neural networks:\n",
    "$$v^{t+1} = \\beta*\\upsilon^{t} + (1-\\beta)*(c1*r1(p_{best} − \\theta^t) + c2*r2(g_{best} − \\theta^t)) − \\alpha*\\nabla Loss(\\theta^t)$$\n",
    "where $\\alpha$ is *lr*.\n",
    "\n",
    "Based on formulaes above, here is matching formulaes coef-nts with *PSO* algorithm parameters:\n",
    "1. pop_size (int, optional): The swarm. Defaults to 30.\n",
    "2. b (float, optional): Inertia of the particles. Defaults to 0.9.\n",
    "3. c1 (float, optional): The *p-best* coeficient. Defaults to 0.08.\n",
    "4. c2 (float, optional): The *g-best* coeficient. Defaults to 0.5.\n",
    "5. c_decrease (bool, optional): Flag for update_pso_params method. Defautls to False.\n",
    "6. variance (float, optional): Variance parameter for swarm creation\n",
    "based on init model, ifluence on r1 and r2 coeff-nts. Defaults to 1.\n",
    "7. lr (float, optional): Learning rate for gradient descent. Defaults to 1e-3.\n",
    "    If 0, there will be only PSO optimization without gradients.\n",
    "8. epsilon (float, optional): some add to gradient descent like in Adam optimizer. Defaults to 1e-8.\n",
    "\n",
    "After preliminaries, to sart solving the equation, we should call Solver class with method solve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-23 18:27:25.861406] initial (min) loss is 1.9947898387908936\n",
      "[2023-11-23 18:27:25.873439] Print every 1000 step\n",
      "Step = 0 loss = 1.994790 normalized loss line= -0.000000x+1.000000. There was 0 stop dings already.\n",
      "[2023-11-23 18:27:35.200447] Print every 1000 step\n",
      "Step = 1000 loss = 0.240816 normalized loss line= -0.000271x+1.026659. There was 0 stop dings already.\n",
      "[2023-11-23 18:27:44.604552] Print every 1000 step\n",
      "Step = 2000 loss = 0.173032 normalized loss line= -0.000313x+1.030409. There was 0 stop dings already.\n",
      "[2023-11-23 18:27:53.946474] Print every 1000 step\n",
      "Step = 3000 loss = 0.036074 normalized loss line= -0.003048x+1.291535. There was 0 stop dings already.\n",
      "[2023-11-23 18:28:03.309823] Print every 1000 step\n",
      "Step = 4000 loss = 0.012178 normalized loss line= -0.000818x+1.092276. There was 0 stop dings already.\n",
      "[2023-11-23 18:28:12.676755] Print every 1000 step\n",
      "Step = 5000 loss = 0.007998 normalized loss line= -0.000253x+1.024907. There was 0 stop dings already.\n",
      "[2023-11-23 18:28:22.148088] Print every 1000 step\n",
      "Step = 6000 loss = 0.006179 normalized loss line= -0.000317x+1.031618. There was 0 stop dings already.\n",
      "[2023-11-23 18:28:31.578200] Print every 1000 step\n",
      "Step = 7000 loss = 0.003707 normalized loss line= -0.003056x+1.232470. There was 0 stop dings already.\n",
      "[2023-11-23 18:28:40.989101] Print every 1000 step\n",
      "Step = 8000 loss = 0.002276 normalized loss line= -0.000483x+1.128642. There was 0 stop dings already.\n",
      "[2023-11-23 18:28:50.446696] Print every 1000 step\n",
      "Step = 9000 loss = 0.001617 normalized loss line= -0.001179x+1.090750. There was 0 stop dings already.\n",
      "[2023-11-23 18:28:59.802114] Print every 1000 step\n",
      "Step = 10000 loss = 0.001300 normalized loss line= -0.001302x+1.208620. There was 0 stop dings already.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6388\\2125334694.py:17: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  return -quad(integrand1, -np.inf, np.inf, args=(x, t))[0] / quad(integrand2, -np.inf, np.inf, args=(x, t))[\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6388\\2125334694.py:17: IntegrationWarning: The integral is probably divergent, or slowly convergent.\n",
      "  return -quad(integrand1, -np.inf, np.inf, args=(x, t))[0] / quad(integrand2, -np.inf, np.inf, args=(x, t))[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE_grad=  0.02456929041361502\n"
     ]
    }
   ],
   "source": [
    "img_dir=os.path.join(os.path.dirname('tutorials'), 'Burg_eq_img')\n",
    "\n",
    "if not(os.path.isdir(img_dir)):\n",
    "    os.mkdir(img_dir)\n",
    "\n",
    "model = Solver(grid, equation, model, mode).solve(\n",
    "    lambda_bound=10,\n",
    "    verbose=True,\n",
    "    learning_rate=1e-3,\n",
    "    use_cache=False,\n",
    "    print_every=1000,\n",
    "    tmax=10000,\n",
    "    patience=5,\n",
    "    optimizer_mode='Adam',\n",
    "    image_save_dir=img_dir)\n",
    "\n",
    "u_exact = exact(grid).to('cuda')\n",
    "\n",
    "u_exact = check_device(u_exact).reshape(-1)\n",
    "\n",
    "u_pred = check_device(model(grid)).reshape(-1)\n",
    "\n",
    "error_rmse = torch.sqrt(torch.sum((u_exact - u_pred)**2)) / torch.sqrt(torch.sum(u_exact**2))\n",
    "\n",
    "print('RMSE_grad= ', error_rmse.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for trained model we want to start PSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom optimizer is activated\n",
      "[2023-11-23 18:29:01.702648] initial (min) loss is 0.001300989417359233\n",
      "[2023-11-23 18:29:02.520403] Print every 100 step\n",
      "Step = 0 loss = 0.002916 normalized loss line= 0.000329x+0.435457. There was 0 stop dings already.\n",
      "[2023-11-23 18:30:24.662127] Oscillation near the same loss\n",
      "[2023-11-23 18:30:24.663128] Print every 100 step\n",
      "Step = 100 loss = 0.002114 normalized loss line= -0.000000x+1.000000. There was 1 stop dings already.\n",
      "[2023-11-23 18:31:45.337057] Print every 100 step\n",
      "Step = 200 loss = 0.001896 normalized loss line= -0.001313x+1.142264. There was 1 stop dings already.\n",
      "[2023-11-23 18:33:13.051870] Print every 100 step\n",
      "Step = 300 loss = 0.001580 normalized loss line= -0.001884x+1.193018. There was 1 stop dings already.\n",
      "[2023-11-23 18:34:41.517000] Print every 100 step\n",
      "Step = 400 loss = 0.001341 normalized loss line= -0.001716x+1.150953. There was 1 stop dings already.\n",
      "[2023-11-23 18:36:07.558068] Print every 100 step\n",
      "Step = 500 loss = 0.001236 normalized loss line= -0.000872x+1.084218. There was 1 stop dings already.\n",
      "[2023-11-23 18:37:34.366944] Print every 100 step\n",
      "Step = 600 loss = 0.001170 normalized loss line= -0.000573x+1.056576. There was 1 stop dings already.\n",
      "[2023-11-23 18:38:56.720184] Print every 100 step\n",
      "Step = 700 loss = 0.001117 normalized loss line= -0.000474x+1.046161. There was 1 stop dings already.\n",
      "[2023-11-23 18:40:22.333362] Print every 100 step\n",
      "Step = 800 loss = 0.001073 normalized loss line= -0.000412x+1.040068. There was 1 stop dings already.\n",
      "[2023-11-23 18:41:49.715923] Print every 100 step\n",
      "Step = 900 loss = 0.001034 normalized loss line= -0.000373x+1.036556. There was 1 stop dings already.\n",
      "[2023-11-23 18:43:16.785838] Print every 100 step\n",
      "Step = 1000 loss = 0.000998 normalized loss line= -0.000360x+1.035523. There was 1 stop dings already.\n",
      "[2023-11-23 18:44:44.622429] Print every 100 step\n",
      "Step = 1100 loss = 0.000964 normalized loss line= -0.000351x+1.034556. There was 1 stop dings already.\n",
      "[2023-11-23 18:46:12.036395] Print every 100 step\n",
      "Step = 1200 loss = 0.000932 normalized loss line= -0.000346x+1.034135. There was 1 stop dings already.\n",
      "[2023-11-23 18:47:39.422867] Print every 100 step\n",
      "Step = 1300 loss = 0.000901 normalized loss line= -0.000343x+1.033853. There was 1 stop dings already.\n",
      "[2023-11-23 18:49:05.092808] Print every 100 step\n",
      "Step = 1400 loss = 0.000871 normalized loss line= -0.000341x+1.033686. There was 1 stop dings already.\n",
      "[2023-11-23 18:50:35.973191] Print every 100 step\n",
      "Step = 1500 loss = 0.000843 normalized loss line= -0.000340x+1.033575. There was 1 stop dings already.\n",
      "[2023-11-23 18:52:04.445382] Print every 100 step\n",
      "Step = 1600 loss = 0.000815 normalized loss line= -0.000339x+1.033473. There was 1 stop dings already.\n",
      "[2023-11-23 18:53:29.240458] Print every 100 step\n",
      "Step = 1700 loss = 0.000788 normalized loss line= -0.000338x+1.033378. There was 1 stop dings already.\n",
      "[2023-11-23 18:55:00.897550] Print every 100 step\n",
      "Step = 1800 loss = 0.000763 normalized loss line= -0.000337x+1.033255. There was 1 stop dings already.\n",
      "[2023-11-23 18:56:32.654299] Print every 100 step\n",
      "Step = 1900 loss = 0.000738 normalized loss line= -0.000336x+1.033145. There was 1 stop dings already.\n",
      "[2023-11-23 18:57:58.612967] Print every 100 step\n",
      "Step = 2000 loss = 0.000714 normalized loss line= -0.000335x+1.033032. There was 1 stop dings already.\n",
      "[2023-11-23 18:59:26.297792] Print every 100 step\n",
      "Step = 2100 loss = 0.000691 normalized loss line= -0.000334x+1.032961. There was 1 stop dings already.\n",
      "[2023-11-23 19:00:54.382346] Print every 100 step\n",
      "Step = 2200 loss = 0.000669 normalized loss line= -0.000333x+1.032906. There was 1 stop dings already.\n",
      "[2023-11-23 19:02:22.856990] Print every 100 step\n",
      "Step = 2300 loss = 0.000647 normalized loss line= -0.000333x+1.032898. There was 1 stop dings already.\n",
      "[2023-11-23 19:03:47.381432] Print every 100 step\n",
      "Step = 2400 loss = 0.000626 normalized loss line= -0.000334x+1.032949. There was 1 stop dings already.\n",
      "[2023-11-23 19:05:13.139893] Print every 100 step\n",
      "Step = 2500 loss = 0.000606 normalized loss line= -0.000335x+1.033071. There was 1 stop dings already.\n",
      "[2023-11-23 19:06:38.174252] Print every 100 step\n",
      "Step = 2600 loss = 0.000586 normalized loss line= -0.000337x+1.033273. There was 1 stop dings already.\n",
      "[2023-11-23 19:08:05.834133] Print every 100 step\n",
      "Step = 2700 loss = 0.000567 normalized loss line= -0.000340x+1.033579. There was 1 stop dings already.\n",
      "[2023-11-23 19:09:30.114832] Print every 100 step\n",
      "Step = 2800 loss = 0.000548 normalized loss line= -0.000344x+1.034016. There was 1 stop dings already.\n",
      "[2023-11-23 19:10:51.460460] Print every 100 step\n",
      "Step = 2900 loss = 0.000530 normalized loss line= -0.000330x+1.032330. There was 1 stop dings already.\n",
      "[2023-11-23 19:12:12.692723] Print every 100 step\n",
      "Step = 3000 loss = 0.000514 normalized loss line= -0.000326x+1.032200. There was 1 stop dings already.\n",
      "RMSE_pso=  0.0317729685970231\n"
     ]
    }
   ],
   "source": [
    "pso = PSO(\n",
    "        pop_size=100,\n",
    "        b=0.5,\n",
    "        c2=0.05,\n",
    "        variance=5e-3,\n",
    "        c_decrease=True,\n",
    "        lr=5e-3\n",
    "    )\n",
    "    \n",
    "model = Solver(grid, equation, model, mode).solve(\n",
    "    lambda_bound=10,\n",
    "    verbose=True,\n",
    "    eps=1e-6,\n",
    "    use_cache=False,\n",
    "    print_every=100,\n",
    "    tmin=100,\n",
    "    tmax=3000,\n",
    "    patience=5,\n",
    "    optimizer_mode=pso,\n",
    "    image_save_dir=img_dir)\n",
    "\n",
    "u_pred = check_device(model(grid)).reshape(-1)\n",
    "\n",
    "error_rmse = torch.sqrt(torch.sum((u_exact - u_pred)**2)) / torch.sqrt(torch.sum(u_exact**2))\n",
    "\n",
    "print('RMSE_pso= ', error_rmse.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
