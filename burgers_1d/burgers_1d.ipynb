{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.numpy.linalg import lstsq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import torch\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.integrate import quad\n",
    "from copy import copy\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('experiments'), '..')))\n",
    "\n",
    "from tedeous.data import Domain, Conditions, Equation\n",
    "from tedeous.model import Model\n",
    "from tedeous.callbacks import early_stopping\n",
    "from tedeous.optimizers.optimizer import Optimizer\n",
    "from tedeous.eval import integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.set_default_device('cuda')\n",
    "torch.set_default_dtype(torch.float64)\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "def replace_none_with_zero(tuple_data):\n",
    "    if isinstance(tuple_data, torch.Tensor):\n",
    "        tuple_data[tuple_data == None] = 0\n",
    "    elif tuple_data is None:\n",
    "        tuple_data = torch.tensor([0.])\n",
    "    elif isinstance(tuple_data, tuple):\n",
    "        new_tuple = tuple(replace_none_with_zero(item) for item in tuple_data)\n",
    "        return new_tuple\n",
    "    return tuple_data\n",
    "\n",
    "def gramian(net, residuals):\n",
    "        # Compute the jacobian on batched data\n",
    "    def jacobian():\n",
    "        jac = []\n",
    "        loss = residuals\n",
    "        for l in loss:\n",
    "            j = torch.autograd.grad(l, net.parameters(), retain_graph=True, allow_unused=True)\n",
    "            j = replace_none_with_zero(j)\n",
    "            j = parameters_to_vector(j).reshape(1, -1)\n",
    "            jac.append(j)\n",
    "        return torch.cat(jac)\n",
    "\n",
    "    J = jacobian()\n",
    "    return 1.0 / len(residuals) * J.T @ J\n",
    "\n",
    "def grid_line_search_factory(loss, steps):\n",
    "\n",
    "    def loss_at_step(step, model, tangent_params):\n",
    "        params = parameters_to_vector(model.parameters())\n",
    "        new_params = params - step*tangent_params\n",
    "        vector_to_parameters(new_params, model.parameters())\n",
    "        loss_val, _ = loss()\n",
    "        vector_to_parameters(params, model.parameters())\n",
    "        return loss_val\n",
    "\n",
    "    def grid_line_search_update(model, tangent_params):\n",
    "\n",
    "        losses = []\n",
    "        for step in steps:\n",
    "            losses.append(loss_at_step(step, model, tangent_params).reshape(1))\n",
    "        losses = torch.cat(losses)\n",
    "        step_size = steps[torch.argmin(losses)]\n",
    "\n",
    "        params = parameters_to_vector(model.parameters())\n",
    "        new_params = params - step_size*tangent_params\n",
    "        vector_to_parameters(new_params, model.parameters())\n",
    "\n",
    "        return step_size\n",
    "\n",
    "    return grid_line_search_update\n",
    "\n",
    "\n",
    "def burgers_NGD(grid_res):\n",
    "    start = time.time()\n",
    "    mu = 0.01 / np.pi\n",
    "    l_op = 1\n",
    "    l_bound = 1\n",
    "    grid_steps = torch.linspace(0, 30, 31)\n",
    "    # grid_steps = torch.linspace(0, 10, 11)\n",
    "    steps = 0.5**grid_steps\n",
    "\n",
    "    domain = Domain()\n",
    "    domain.variable('x', [-1, 1], grid_res, dtype='float64')\n",
    "    domain.variable('t', [0, 1], grid_res, dtype='float64')\n",
    "\n",
    "    boundaries = Conditions()\n",
    "    x = domain.variable_dict['x']\n",
    "    boundaries.dirichlet({'x': [-1, 1], 't': 0}, value=-torch.sin(np.pi * x))\n",
    "\n",
    "    boundaries.dirichlet({'x': -1, 't': [0, 1]}, value=0)\n",
    "\n",
    "    boundaries.dirichlet({'x': 1, 't': [0, 1]}, value=0)\n",
    "\n",
    "    equation = Equation()\n",
    "\n",
    "    burgers_eq = {\n",
    "        'du/dt**1':\n",
    "            {\n",
    "                'coeff': 1.,\n",
    "                'du/dt': [1],\n",
    "                'pow': 1,\n",
    "                'var': 0\n",
    "            },\n",
    "        '+u*du/dx':\n",
    "            {\n",
    "                'coeff': 1,\n",
    "                'u*du/dx': [[None], [0]],\n",
    "                'pow': [1, 1],\n",
    "                'var': [0, 0]\n",
    "            },\n",
    "        '-mu*d2u/dx2':\n",
    "            {\n",
    "                'coeff': -mu,\n",
    "                'd2u/dx2': [0, 0],\n",
    "                'pow': 1,\n",
    "                'var': 0\n",
    "            }\n",
    "    }\n",
    "\n",
    "    equation.add(burgers_eq)\n",
    "    \n",
    "    net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(2, 50),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(50, 50),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(50, 1)\n",
    "    )\n",
    "\n",
    "    model = Model(net, domain, equation, boundaries)\n",
    "\n",
    "    model.compile('autograd', lambda_operator=l_op, lambda_bound=l_bound)\n",
    "\n",
    "    ls_update = grid_line_search_factory(model.solution_cls.evaluate, steps)\n",
    "\n",
    "    for iteration in range(200):\n",
    "        loss, _ = model.solution_cls.evaluate()\n",
    "        grads = torch.autograd.grad(loss, model.net.parameters(), retain_graph=True, allow_unused=True)\n",
    "        grads = replace_none_with_zero(grads)\n",
    "        f_grads = parameters_to_vector(grads)\n",
    "\n",
    "        int_res = model.solution_cls.operator._pde_compute()\n",
    "        bval, true_bval, _, _ = model.solution_cls.boundary.apply_bcs()\n",
    "        bound_res = bval-true_bval\n",
    "\n",
    "        # assemble gramian\n",
    "        G_int  = gramian(model.net, int_res)\n",
    "\n",
    "        G_bdry = gramian(model.net, bound_res)\n",
    "        G      = G_int + G_bdry\n",
    "\n",
    "        # Marquardt-Levenberg\n",
    "        Id = torch.eye(len(G))\n",
    "        G = torch.min(torch.tensor([loss, 0.0])) * Id + G\n",
    "        # compute natural gradient\n",
    "        G = jnp.array(G.detach().cpu().numpy(), dtype=jnp.float64)\n",
    "        f_grads =jnp.array(f_grads.detach().cpu().numpy(), dtype=jnp.float64)\n",
    "        f_nat_grad = lstsq(G, f_grads)[0]\n",
    "        f_nat_grad = torch.from_numpy(np.array(f_nat_grad)).to(torch.float64).to('cuda')\n",
    "\n",
    "        # one step of NGD\n",
    "        actual_step = ls_update(model.net, f_nat_grad)\n",
    "        if iteration%5 == 0:\n",
    "            print('iteration= ', iteration)\n",
    "            print('step= ', actual_step.item())\n",
    "            print('loss=' , model.solution_cls.evaluate()[0].item())\n",
    "\n",
    "            grid = domain.build('autograd')\n",
    "            \n",
    "            fig = plt.figure(figsize=(15, 8))\n",
    "            ax1 = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "            ax1.plot_trisurf(grid[:, 0].detach().cpu().numpy(),\n",
    "                                grid[:, 1].detach().cpu().numpy(),\n",
    "                                model.net(grid).reshape(-1).detach().cpu().numpy(),\n",
    "                                cmap=cm.jet, linewidth=0.2, alpha=1)\n",
    "            ax1.set_xlabel(\"x1\")\n",
    "            ax1.set_ylabel(\"x2\")\n",
    "            plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "burgers_NGD(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-27 21:57:28.907124] initial (min) loss is 0.07987409085035324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18896\\566340818.py:157: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  return -quad(integrand1, -np.inf, np.inf, args=(x, t))[0] / quad(integrand2, -np.inf, np.inf, args=(x, t))[\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18896\\566340818.py:157: IntegrationWarning: The integral is probably divergent, or slowly convergent.\n",
      "  return -quad(integrand1, -np.inf, np.inf, args=(x, t))[0] / quad(integrand2, -np.inf, np.inf, args=(x, t))[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-27 22:01:27.787118] initial (min) loss is 0.0004926200490444899\n",
      "Time taken 50= 12188.649454832077\n",
      "RMSE_adam 50= 0.026755044264240954\n",
      "RMSE_pso 50= 0.014638449019887832\n",
      "[2024-03-28 01:24:42.231384] initial (min) loss is 0.09667731821537018\n",
      "[2024-03-28 01:29:47.302618] initial (min) loss is 0.0002246783406008035\n",
      "Time taken 50= 11376.053540706635\n",
      "RMSE_adam 50= 0.019162391230002095\n",
      "RMSE_pso 50= 0.007990365406405069\n",
      "[2024-03-28 04:39:29.140509] initial (min) loss is 0.11380793154239655\n",
      "[2024-03-28 04:45:33.738007] initial (min) loss is 0.0016877059824764729\n",
      "Time taken 50= 11272.672253847122\n",
      "RMSE_adam 50= 0.15236769739404457\n",
      "RMSE_pso 50= 0.14533534527130804\n",
      "[2024-03-28 07:53:32.160141] initial (min) loss is 0.086567722260952\n",
      "[2024-03-28 07:58:50.862815] initial (min) loss is 0.0007062613149173558\n",
      "Time taken 50= 11327.171834230423\n",
      "RMSE_adam 50= 0.03429887425848642\n",
      "RMSE_pso 50= 0.016316543119122145\n",
      "[2024-03-28 11:07:43.795175] initial (min) loss is 0.08707326650619507\n",
      "[2024-03-28 11:12:36.678133] initial (min) loss is 0.0007530385628342628\n",
      "Time taken 50= 12544.625535488129\n",
      "RMSE_adam 50= 0.03559135062044764\n",
      "RMSE_pso 50= 0.016203584534874152\n",
      "[2024-03-28 14:41:47.538738] initial (min) loss is 0.09608609229326248\n",
      "[2024-03-28 14:46:31.807216] initial (min) loss is 0.0009523407206870615\n",
      "Time taken 60= 14922.736966609955\n",
      "RMSE_adam 60= 0.05695456197190428\n",
      "RMSE_pso 60= 0.048788113509813505\n",
      "[2024-03-28 18:55:23.098424] initial (min) loss is 0.09206864982843399\n",
      "[2024-03-28 19:01:51.916564] initial (min) loss is 0.0003464377950876951\n",
      "Time taken 60= 12355.13696551323\n",
      "RMSE_adam 60= 0.03365704167127067\n",
      "RMSE_pso 60= 0.025732049674617556\n",
      "[2024-03-28 22:27:55.155157] initial (min) loss is 0.12094864249229431\n",
      "[2024-03-28 22:31:10.847924] initial (min) loss is 0.0017648825887590647\n",
      "Time taken 60= 11964.229009389877\n",
      "RMSE_adam 60= 0.06196593779890389\n",
      "RMSE_pso 60= 0.033890226097937855\n",
      "[2024-03-29 01:50:43.090904] initial (min) loss is 0.1088319644331932\n",
      "[2024-03-29 01:53:34.870194] initial (min) loss is 0.0015544933266937733\n",
      "Time taken 60= 12468.155700445175\n",
      "RMSE_adam 60= 0.05937883574497447\n",
      "RMSE_pso 60= 0.03375706790857142\n",
      "[2024-03-29 05:21:30.968190] initial (min) loss is 0.14027690887451172\n",
      "[2024-03-29 05:27:24.335017] initial (min) loss is 0.000758515321649611\n",
      "Time taken 60= 12280.673333406448\n",
      "RMSE_adam 60= 0.046995795979624964\n",
      "RMSE_pso 60= 0.027116208718100172\n",
      "[2024-03-29 08:52:13.028677] initial (min) loss is 0.07647019624710083\n",
      "[2024-03-29 08:59:50.472597] initial (min) loss is 0.0006360391853377223\n",
      "Time taken 70= 15643.945709228516\n",
      "RMSE_adam 70= 0.08208982986592382\n",
      "RMSE_pso 70= 0.07792143037244477\n",
      "[2024-03-29 13:20:45.197515] initial (min) loss is 0.12275654077529907\n",
      "[2024-03-29 13:25:28.904442] initial (min) loss is 0.0015605511143803596\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 173\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grid_res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m71\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nruns):\n\u001b[1;32m--> 173\u001b[0m         exp_dict_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43msolver_burgers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_res\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    175\u001b[0m exp_dict_list_flatten \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m exp_dict_list \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n\u001b[0;32m    176\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(exp_dict_list_flatten)\n",
      "Cell \u001b[1;32mIn[4], line 101\u001b[0m, in \u001b[0;36msolver_burgers\u001b[1;34m(grid_res)\u001b[0m\n\u001b[0;32m     94\u001b[0m optim \u001b[38;5;241m=\u001b[39m Optimizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPSO\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpop_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m50\u001b[39m, \u001b[38;5;66;03m#30\u001b[39;00m\n\u001b[0;32m     95\u001b[0m                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;66;03m#0.5\u001b[39;00m\n\u001b[0;32m     96\u001b[0m                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;66;03m#0.05\u001b[39;00m\n\u001b[0;32m     97\u001b[0m                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m, \n\u001b[0;32m     98\u001b[0m                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariance\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5e-2\u001b[39m,\n\u001b[0;32m     99\u001b[0m                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-4\u001b[39m})\n\u001b[0;32m    100\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 101\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2e4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcb_es\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    103\u001b[0m time_pso \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\ITMO\\solver_aminevdam\\torch_DE_solver\\tedeous\\model.py:162\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, optimizer, epochs, info_string_every, mixed_precision, save_model, model_name, callbacks)\u001b[0m\n\u001b[0;32m    160\u001b[0m     closure()\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m%\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mdecay_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    164\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ITMO\\solver_aminevdam\\torch_DE_solver\\tedeous\\optimizers\\pso.py:189\u001b[0m, in \u001b[0;36mPSO.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" It runs ONE step on the particle swarm optimization.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m        torch.Tensor: loss value for best particle of thw swarm.\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_swarm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrads_swarm \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_p \u001b[38;5;241m=\u001b[39m copy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_swarm)\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\ITMO\\solver_aminevdam\\torch_DE_solver\\tedeous\\optimizers\\closure.py:87\u001b[0m, in \u001b[0;36mClosure._closure_pso\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m particle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mswarm:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mvec_to_params(particle)\n\u001b[1;32m---> 87\u001b[0m     loss_particle, grads \u001b[38;5;241m=\u001b[39m \u001b[43mloss_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     loss_swarm\u001b[38;5;241m.\u001b[39mappend(loss_particle)\n\u001b[0;32m     89\u001b[0m     grads_swarm\u001b[38;5;241m.\u001b[39mappend(grads\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\ITMO\\solver_aminevdam\\torch_DE_solver\\tedeous\\optimizers\\closure.py:73\u001b[0m, in \u001b[0;36mClosure._closure_pso.<locals>.loss_grads\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m     71\u001b[0m                     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m     72\u001b[0m                     enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixed_precision):\n\u001b[1;32m---> 73\u001b[0m     loss, loss_normalized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolution_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39muse_grad:\n\u001b[0;32m     76\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mgradient(loss)\n",
      "File \u001b[1;32mc:\\ITMO\\solver_aminevdam\\torch_DE_solver\\tedeous\\solution.py:133\u001b[0m, in \u001b[0;36mSolution.evaluate\u001b[1;34m(self, save_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    117\u001b[0m              save_graph: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Computes loss.\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m        Tuple[torch.Tensor, torch.Tensor]: loss\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperator_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbval, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_bval,\\\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbval_keys, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbval_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboundary\u001b[38;5;241m.\u001b[39mapply_bcs()\n\u001b[0;32m    136\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[1;32mc:\\ITMO\\solver_aminevdam\\torch_DE_solver\\tedeous\\eval.py:203\u001b[0m, in \u001b[0;36mOperator.operator_compute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Corresponding to form (weak or strong) calculate residual of operator.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m    torch.Tensor: operator residual.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweak_form \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweak_form \u001b[38;5;241m==\u001b[39m []:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pde_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weak_pde_compute()\n",
      "File \u001b[1;32mc:\\ITMO\\solver_aminevdam\\torch_DE_solver\\tedeous\\eval.py:157\u001b[0m, in \u001b[0;36mOperator._pde_compute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    155\u001b[0m num_of_eq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepared_operator)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_of_eq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 157\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_operator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepared_operator\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msorted_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    160\u001b[0m     op_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\ITMO\\solver_aminevdam\\torch_DE_solver\\tedeous\\eval.py:141\u001b[0m, in \u001b[0;36mOperator.apply_operator\u001b[1;34m(self, operator, grid_points)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m operator:\n\u001b[0;32m    140\u001b[0m     term \u001b[38;5;241m=\u001b[39m operator[term]\n\u001b[1;32m--> 141\u001b[0m     dif \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mderivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dif\n",
      "File \u001b[1;32mc:\\ITMO\\solver_aminevdam\\torch_DE_solver\\tedeous\\derivative.py:121\u001b[0m, in \u001b[0;36mDerivative_autograd.take_derivative\u001b[1;34m(self, term, grid_points)\u001b[0m\n\u001b[0;32m    119\u001b[0m         der \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(grid_points)[:, term[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar\u001b[39m\u001b[38;5;124m'\u001b[39m][j]]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 121\u001b[0m         der \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mderivative\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     der_term \u001b[38;5;241m=\u001b[39m der_term \u001b[38;5;241m*\u001b[39m der \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m term[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpow\u001b[39m\u001b[38;5;124m'\u001b[39m][j]\n\u001b[0;32m    124\u001b[0m der_term \u001b[38;5;241m=\u001b[39m coeff \u001b[38;5;241m*\u001b[39m der_term\n",
      "File \u001b[1;32mc:\\ITMO\\solver_aminevdam\\torch_DE_solver\\tedeous\\derivative.py:90\u001b[0m, in \u001b[0;36mDerivative_autograd._nn_autograd\u001b[1;34m(model, points, var, axis)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Computes derivative on the grid using autograd method.\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m        in corresponding axis.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m points\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m fi \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m)\u001b[49m[:, var]\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axis:\n\u001b[0;32m     92\u001b[0m     grads, \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(fi, points, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\activation.py:359\u001b[0m, in \u001b[0;36mTanh.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def solver_burgers(grid_res):\n",
    "    exp_dict_list = []\n",
    "    start = time.time()\n",
    "    mu = 0.01 / np.pi\n",
    "\n",
    "    domain = Domain()\n",
    "    domain.variable('x', [-1, 1], grid_res)\n",
    "    domain.variable('t', [0, 1], grid_res)\n",
    "\n",
    "    boundaries = Conditions()\n",
    "    x = domain.variable_dict['x']\n",
    "    boundaries.dirichlet({'x': [-1, 1], 't': 0}, value=-torch.sin(np.pi * x))\n",
    "\n",
    "    boundaries.dirichlet({'x': -1, 't': [0, 1]}, value=0)\n",
    "\n",
    "    boundaries.dirichlet({'x': 1, 't': [0, 1]}, value=0)\n",
    "\n",
    "    equation = Equation()\n",
    "\n",
    "    burgers_eq = {\n",
    "        'du/dt**1':\n",
    "            {\n",
    "                'coeff': 1.,\n",
    "                'du/dt': [1],\n",
    "                'pow': 1,\n",
    "                'var': 0\n",
    "            },\n",
    "        '+u*du/dx':\n",
    "            {\n",
    "                'coeff': 1,\n",
    "                'u*du/dx': [[None], [0]],\n",
    "                'pow': [1, 1],\n",
    "                'var': [0, 0]\n",
    "            },\n",
    "        '-mu*d2u/dx2':\n",
    "            {\n",
    "                'coeff': -mu,\n",
    "                'd2u/dx2': [0, 0],\n",
    "                'pow': 1,\n",
    "                'var': 0\n",
    "            }\n",
    "    }\n",
    "\n",
    "    equation.add(burgers_eq)\n",
    "\n",
    "    net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(2, 32),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(32, 32),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(32, 1)\n",
    "    )\n",
    "\n",
    "    model = Model(net, domain, equation, boundaries)\n",
    "\n",
    "    model.compile('autograd', lambda_operator=1/2, lambda_bound=1/2)\n",
    "\n",
    "    cb_es = early_stopping.EarlyStopping(eps=1e-6,\n",
    "                                        loss_window=100,\n",
    "                                        no_improvement_patience=100,\n",
    "                                        patience=2,\n",
    "                                        randomize_parameter=1e-5,\n",
    "                                        verbose=False)\n",
    "\n",
    "    optim = Optimizer('Adam', {'lr': 1e-3})\n",
    "    model.train(optim, 2e5, callbacks=[cb_es])\n",
    "    end = time.time()\n",
    "\n",
    "    time_adam = end - start\n",
    "\n",
    "    grid = domain.build('autograd')\n",
    "\n",
    "    u_exact = exact(grid).reshape(-1)\n",
    "\n",
    "    error_adam = torch.sqrt(torch.mean((u_exact - net(grid).reshape(-1)) ** 2))\n",
    "\n",
    "    loss_adam = model.solution_cls.evaluate()[0].detach().cpu().numpy()\n",
    "\n",
    "    lu_f = model.solution_cls.operator.operator_compute()\n",
    "\n",
    "    lu_f, gr = integration(lu_f, grid)\n",
    "\n",
    "    lu_f_adam, _ = integration(lu_f, gr)\n",
    "\n",
    "    ########\n",
    "\n",
    "    cb_es = early_stopping.EarlyStopping(eps=1e-6,\n",
    "                                        loss_window=100,\n",
    "                                        no_improvement_patience=100,\n",
    "                                        patience=2,\n",
    "                                        randomize_parameter=1e-5,\n",
    "                                        verbose=False)\n",
    "\n",
    "    optim = Optimizer('PSO', {'pop_size': 50, #30\n",
    "                                  'b': 0.4, #0.5\n",
    "                                  'c2': 0.5, #0.05\n",
    "                                  'c1': 0.5, \n",
    "                                  'variance': 5e-2,\n",
    "                                  'lr': 1e-4})\n",
    "    start = time.time()\n",
    "    model.train(optim, 2e4, save_model=False, callbacks=[cb_es])\n",
    "    end = time.time()\n",
    "    time_pso = end - start\n",
    "\n",
    "    u_exact = exact(grid).reshape(-1)\n",
    "\n",
    "    error_pso = torch.sqrt(torch.mean((u_exact - net(grid).reshape(-1)) ** 2))\n",
    "\n",
    "    loss_pso = model.solution_cls.evaluate()[0].detach().cpu().numpy()\n",
    "\n",
    "    lu_f = model.solution_cls.operator.operator_compute()\n",
    "\n",
    "    grid = domain.build('autograd')\n",
    "\n",
    "    lu_f, gr = integration(lu_f, grid)\n",
    "\n",
    "    lu_f_pso, _ = integration(lu_f, gr)\n",
    "\n",
    "    #########\n",
    "\n",
    "    \n",
    "\n",
    "    exp_dict_list.append({'grid_res': grid_res,\n",
    "                          'error_adam': error_adam.item(),\n",
    "                          'error_PSO': error_pso.item(),\n",
    "                          'loss_adam': loss_adam.item(),\n",
    "                          'loss_pso': loss_pso.item(),\n",
    "                          \"lu_f_adam\": lu_f_adam.item(),\n",
    "                          \"lu_f_pso\": lu_f_pso.item(),\n",
    "                          'time_adam': time_adam,\n",
    "                          'time_pso': time_pso,\n",
    "                          'type':'Burgers'})\n",
    "\n",
    "    print('Time taken {}= {}'.format(grid_res, end - start))\n",
    "    print('RMSE_adam {}= {}'.format(grid_res, error_adam))\n",
    "    print('RMSE_pso {}= {}'.format(grid_res, error_pso))\n",
    "\n",
    "    return exp_dict_list\n",
    "\n",
    "\n",
    "def exact(grid):\n",
    "    mu = 0.01 / np.pi\n",
    "\n",
    "    def f(y):\n",
    "        return np.exp(-np.cos(np.pi * y) / (2 * np.pi * mu))\n",
    "\n",
    "    def integrand1(m, x, t):\n",
    "        return np.sin(np.pi * (x - m)) * f(x - m) * np.exp(-m ** 2 / (4 * mu * t))\n",
    "\n",
    "    def integrand2(m, x, t):\n",
    "        return f(x - m) * np.exp(-m ** 2 / (4 * mu * t))\n",
    "\n",
    "    def u(x, t):\n",
    "        if t == 0:\n",
    "            return -np.sin(np.pi * x)\n",
    "        else:\n",
    "            return -quad(integrand1, -np.inf, np.inf, args=(x, t))[0] / quad(integrand2, -np.inf, np.inf, args=(x, t))[\n",
    "                0]\n",
    "\n",
    "    solution = []\n",
    "    for point in grid:\n",
    "        solution.append(u(point[0].item(), point[1].item()))\n",
    "\n",
    "    return torch.tensor(solution)\n",
    "\n",
    "\n",
    "nruns = 5\n",
    "###########################\n",
    "exp_dict_list = []\n",
    "\n",
    "for grid_res in range(50, 71, 10):\n",
    "    for _ in range(nruns):\n",
    "        exp_dict_list.append(solver_burgers(grid_res))\n",
    "\n",
    "exp_dict_list_flatten = [item for sublist in exp_dict_list for item in sublist]\n",
    "df = pd.DataFrame(exp_dict_list_flatten)\n",
    "df.to_csv('burgers_10_100_adam_pso={}.csv')\n",
    "###########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dict_list_flatten = [item for sublist in exp_dict_list for item in sublist]\n",
    "df = pd.DataFrame(exp_dict_list_flatten)\n",
    "df.to_csv('burgers_50_70_adam_pso={}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
